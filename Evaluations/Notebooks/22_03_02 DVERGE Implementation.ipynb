{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/ubuntu/FedEM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import General Libraries\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import FedEM based Libraries\n",
    "from utils.utils import *\n",
    "from utils.constants import *\n",
    "from utils.args import *\n",
    "from run_experiment import *\n",
    "from models import *\n",
    "\n",
    "# Import Transfer Attack\n",
    "from transfer_attacks.Personalized_NN import *\n",
    "from transfer_attacks.Params import *\n",
    "from transfer_attacks.Transferer import *\n",
    "from transfer_attacks.Args import *\n",
    "from transfer_attacks.TA_utils import *\n",
    "from transfer_attacks.Boundary_Transferer import *\n",
    "\n",
    "from client import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adv_MixtureClient_DVERGE(MixtureClient):\n",
    "    \"\"\" \n",
    "    ADV client with more params -- use to PGD generate data between rounds\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            learners_ensemble,\n",
    "            train_iterator,\n",
    "            val_iterator,\n",
    "            test_iterator,\n",
    "            logger,\n",
    "            local_steps,\n",
    "            tune_locally=False,\n",
    "            dataset_name = 'cifar10'\n",
    "    ):\n",
    "        super(Adv_MixtureClient, self).__init__(\n",
    "            learners_ensemble=learners_ensemble,\n",
    "            train_iterator=train_iterator,\n",
    "            val_iterator=val_iterator,\n",
    "            test_iterator=test_iterator,\n",
    "            logger=logger,\n",
    "            local_steps=local_steps,\n",
    "            tune_locally=tune_locally\n",
    "        )\n",
    "\n",
    "        self.adv_proportion = 0\n",
    "        self.atk_params = None\n",
    "        \n",
    "        # Make copy of dataset and set aside for adv training\n",
    "        self.og_dataloader = deepcopy(self.train_iterator) # Update self.train_loader every iteration\n",
    "        \n",
    "        # Add adversarial client \n",
    "        self.altered_dataloader = self.gen_customdataloader(self.og_dataloader)\n",
    "        self.adv_nns = update_advnn()\n",
    "        \n",
    "        self.dataset_name = dataset_name\n",
    "        self.num_hypotheses = len(self.learners_ensemble.learners)\n",
    "        \n",
    "        self.train_iterator_list = []\n",
    "    \n",
    "    def set_adv_params(self, adv_proportion = 0, atk_params = None):\n",
    "        self.adv_proportion = adv_proportion\n",
    "        self.atk_params = atk_params\n",
    "    \n",
    "    def gen_customdataloader(self, og_dataloader):\n",
    "        # Combine Validation Data across all clients as test\n",
    "        data_x = []\n",
    "        data_y = []\n",
    "\n",
    "        for (x,y,idx) in og_dataloader.dataset:\n",
    "            data_x.append(x)\n",
    "            data_y.append(y)\n",
    "\n",
    "        data_x = torch.stack(data_x)\n",
    "        try:\n",
    "            data_y = torch.stack(data_y)\n",
    "        except:\n",
    "            data_y = torch.tensor(data_y)\n",
    "        dataloader = Custom_Dataloader(data_x, data_y)\n",
    "        \n",
    "        return dataloader\n",
    "    \n",
    "    def update_advnn(self):\n",
    "        # reassign weights after trained\n",
    "        # make X adv nn based on the number of hypotheses that are present at the models\n",
    "        for i in range(self.num_hypotheses):\n",
    "            self.adv_nns[i] = Adv_NN(self.learners_ensemble.learners[i].model, self.altered_dataloader)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def generate_adversarial_data(self):\n",
    "        # Generate adversarial datapoints while recognizing idx of sampled without replacement\n",
    "        \n",
    "        # Draw random idx without replacement \n",
    "        num_datapoints = self.train_iterator.dataset.targets.shape[0]\n",
    "        sample_size = int(np.ceil(num_datapoints * self.adv_proportion))\n",
    "        sample = np.random.choice(a=num_datapoints, size=sample_size)\n",
    "        \n",
    "        # Sample the proportion and split the sampled dataset into N equal groups per hypotheses\n",
    "        sample_groups = {}\n",
    "        x_adv_groups = {} # Splitting by hypotheses\n",
    "        \n",
    "        for i in range(self.num_hypotheses):\n",
    "            sub_sample = sample[np.floor(i*sample.shape[0]/self.num_hypotheses):\n",
    "                                np.floor((i+1)*sample.shape[0]/self.num_hypotheses)]\n",
    "            sample_groups[i] = sub_sample\n",
    "            x_data = self.adv_nn.dataloader.x_data[sub_sample]\n",
    "            y_data = self.adv_nn.dataloader.y_data[sub_sample]\n",
    "            \n",
    "            \n",
    "            self.adv_nns[i].pgd_sub(self.atk_params, x_data.cuda(), y_data.cuda())\n",
    "            x_adv_groups[i] = self.adv_nns[i].x_adv\n",
    "        \n",
    "        return sample_groups, x_adv_groups\n",
    "    \n",
    "    def assign_advdataset(self):\n",
    "        # convert dataset to normed and replace specific datapoints\n",
    "        \n",
    "        # Flush current used dataset with original\n",
    "        self.train_iterator = deepcopy(self.og_dataloader)\n",
    "        train_iterator_list = []\n",
    "        \n",
    "        # adversarial datasets loop, adjust normed and push \n",
    "        sample_id_groups, x_adv_groups = self.generate_adversarial_data()\n",
    "        \n",
    "        for j in range(self.num_hypotheses):\n",
    "            train_iterator_list += [deepcopy(self.og_dataloader)]\n",
    "            \n",
    "            for k in range(self.num_hypotheses):\n",
    "                if j != k:\n",
    "                    sample_id = sample_id_groups[k]\n",
    "                    x_adv = x_adv_groups[k]\n",
    "\n",
    "                    for i in range(sample_id.shape[0]):\n",
    "                        idx = sample_id[i]\n",
    "                        x_val_normed = x_adv[i]\n",
    "                        if self.dataset_name == 'cifar10' or self.dataset_name == 'cifar100':\n",
    "                            x_val_unnorm = unnormalize_cifar10(x_val_normed)\n",
    "                        elif self.dataset_name == 'mnist' or self.dataset_name == 'femnist':\n",
    "                            x_val_unnorm = unnormalize_femnist(x_val_normed)\n",
    "                        else:\n",
    "                            print(\"Error: Dataset not recognized\")\n",
    "\n",
    "                        train_iterator_list[j].dataset.data[idx] = x_val_unnorm\n",
    "        \n",
    "        self.train_iterator_list = train_iterator_list\n",
    "        self.train_loader = iter(self.train_iterator)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def step(self, single_batch_flag=False, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        perform on step for the client\n",
    "\n",
    "        :param single_batch_flag: if true, the client only uses one batch to perform the update\n",
    "        :return\n",
    "            clients_updates: ()\n",
    "        \"\"\"\n",
    "        self.counter += 1\n",
    "        self.update_sample_weights()\n",
    "        self.update_learners_weights()\n",
    "\n",
    "        if single_batch_flag:\n",
    "            batch = self.get_next_batch()\n",
    "            client_updates = \\\n",
    "                self.learners_ensemble.fit_batch(\n",
    "                    batch=batch,\n",
    "                    weights=self.samples_weights\n",
    "                )\n",
    "        else:\n",
    "            if len(self.train_iterator_list) == 0:\n",
    "                client_updates = \\\n",
    "                    self.learners_ensemble.fit_epochs(\n",
    "                        iterator=self.train_iterator,\n",
    "                        n_epochs=self.local_steps,\n",
    "                        weights=self.samples_weights)\n",
    "            else:\n",
    "                client_updates = \\\n",
    "                    self.learners_ensemble.fit_epochs_multiple_iterators(\n",
    "                        iterator=self.train_iterator_list,\n",
    "                        n_epochs=self.local_steps,\n",
    "                        weights=self.samples_weights)\n",
    "                \n",
    "        return client_updtes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedEM_env",
   "language": "python",
   "name": "fedem_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
