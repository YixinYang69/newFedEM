{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/ubuntu/FedEM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import General Libraries\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "# Import FedEM based Libraries\n",
    "from utils.utils import *\n",
    "from utils.constants import *\n",
    "from utils.args import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from run_experiment import *\n",
    "from models import *\n",
    "\n",
    "# Import Transfer Attack\n",
    "from transfer_attacks.Personalized_NN import *\n",
    "from transfer_attacks.Params import *\n",
    "from transfer_attacks.Transferer import *\n",
    "from transfer_attacks.Args import *\n",
    "\n",
    "from transfer_attacks.TA_utils import *\n",
    "from transfer_attacks.Boundary_Transferer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually set argument parameters\n",
    "args_ = Args()\n",
    "args_.experiment = \"cifar10\"\n",
    "args_.method = \"FedEM_adv\"\n",
    "args_.decentralized = False\n",
    "args_.sampling_rate = 1.0\n",
    "args_.input_dimension = None\n",
    "args_.output_dimension = None\n",
    "args_.n_learners= 3\n",
    "args_.n_rounds = 10\n",
    "args_.bz = 128\n",
    "args_.local_steps = 1\n",
    "args_.lr_lambda = 0\n",
    "args_.lr =0.03\n",
    "args_.lr_scheduler = 'multi_step'\n",
    "args_.log_freq = 10\n",
    "args_.device = 'cuda'\n",
    "args_.optimizer = 'sgd'\n",
    "args_.mu = 0\n",
    "args_.communication_probability = 0.1\n",
    "args_.q = 1\n",
    "args_.locally_tune_clients = False\n",
    "args_.seed = 1234\n",
    "args_.verbose = 1\n",
    "args_.save_path = 'weights/cifar/22_01_09_fedavg_n80_benign/'\n",
    "args_.validation = False\n",
    "args_.num_user = 40\n",
    "\n",
    "# Generate the dummy values here\n",
    "aggregator, clients = dummy_aggregator(args_, num_user=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Validation Data across all clients as test\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "for i in range(len(clients)):\n",
    "    daniloader = clients[i].val_iterator\n",
    "    for (x,y,idx) in daniloader.dataset:\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "\n",
    "data_x = torch.stack(data_x)\n",
    "data_y = torch.stack(data_y)\n",
    "\n",
    "# Create dataloader from validation dataset that allows for diverse batch size\n",
    "dataloader = Custom_Dataloader(data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Model Weights\n",
    "setting = 'FedAvg'\n",
    "\n",
    "if setting == 'FedEM':\n",
    "    nL = 3\n",
    "else:\n",
    "    nL = 1\n",
    "\n",
    "adv_mode = False    \n",
    "\n",
    "num_models = 40\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "\n",
    "if setting == 'local':\n",
    "\n",
    "    if adv_mode:\n",
    "        args_.save_path ='weights/cifar100/feddef_l2/local/'\n",
    "        aggregator.load_state(args_.save_path)\n",
    "\n",
    "        model_weights = []\n",
    "        weights = np.load('weights/cifar100/feddef_l2/local/train_client_weights.npy')\n",
    "    else:\n",
    "        args_.save_path ='weights/cifar100/first_test/local/'\n",
    "        aggregator.load_state(args_.save_path)\n",
    "\n",
    "        model_weights = []\n",
    "        weights = np.load('weights/cifar100/first_test/local/train_client_weights.npy')\n",
    "    \n",
    "    for i in range(num_models):\n",
    "        model_weights += [weights[i]]\n",
    "\n",
    "    # Generate the weights to test on as linear combinations of the model_weights\n",
    "    models_test = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        new_model = copy.deepcopy(aggregator.clients[i].learners_ensemble.learners[0].model)\n",
    "        new_model.eval()\n",
    "        models_test += [new_model]\n",
    "\n",
    "elif setting == 'FedAvg':\n",
    "    \n",
    "    \n",
    "    if adv_mode:\n",
    "        args_.save_path = 'weights/cifar100/feddef_l2/fedavg/'\n",
    "    else:\n",
    "        args_.save_path = 'weights/cifar100/first_test/fedavg/'\n",
    "    aggregator.load_state(args_.save_path)\n",
    "    \n",
    "    # This is where the models are stored -- one for each mixture --> learner.model for nn\n",
    "    hypotheses = aggregator.global_learners_ensemble.learners\n",
    "\n",
    "    # obtain the state dict for each of the weights \n",
    "    weights_h = []\n",
    "\n",
    "    for h in hypotheses:\n",
    "        weights_h += [h.model.state_dict()]\n",
    "    \n",
    "    if adv_mode:\n",
    "        weights = np.load('weights/cifar100/feddef_l2/fedavg/train_client_weights.npy')\n",
    "    else:\n",
    "        weights = np.load('weights/cifar100/first_test/fedavg/train_client_weights.npy')\n",
    "    \n",
    "    # Set model weights\n",
    "    model_weights = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        model_weights += [weights[i]]\n",
    "\n",
    "    # Generate the weights to test on as linear combinations of the model_weights\n",
    "    models_test = []\n",
    "\n",
    "    for (w0) in model_weights:\n",
    "        # first make the model with empty weights\n",
    "        new_model = copy.deepcopy(hypotheses[0].model)\n",
    "        new_model.eval()\n",
    "        new_weight_dict = copy.deepcopy(weights_h[0])\n",
    "        for key in weights_h[0]:\n",
    "            new_weight_dict[key] = w0[0]*weights_h[0][key] \n",
    "        new_model.load_state_dict(new_weight_dict)\n",
    "        models_test += [new_model]\n",
    "\n",
    "elif setting == 'FedEM':\n",
    "    \n",
    "    if adv_mode:\n",
    "        args_.save_path = 'weights/neurips/cifar/krum/just_attack/adv/'\n",
    "    else:\n",
    "        args_.save_path = 'weights/neurips/cifar/krum/just_attack/benign/'\n",
    "    aggregator.load_state(args_.save_path)\n",
    "    \n",
    "    # This is where the models are stored -- one for each mixture --> learner.model for nn\n",
    "    hypotheses = aggregator.global_learners_ensemble.learners\n",
    "\n",
    "    # obtain the state dict for each of the weights \n",
    "    weights_h = []\n",
    "\n",
    "    for h in hypotheses:\n",
    "        weights_h += [h.model.state_dict()]\n",
    "\n",
    "    if adv_mode:\n",
    "        weights = np.load('weights/neurips/cifar/krum/just_attack/adv/train_client_weights.npy')\n",
    "    else:\n",
    "        weights = np.load('weights/neurips/cifar/krum/just_attack/benign/train_client_weights.npy')\n",
    "\n",
    "    # Set model weights\n",
    "    model_weights = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        model_weights += [weights[i]]\n",
    "\n",
    "\n",
    "    # Generate the weights to test on as linear combinations of the model_weights\n",
    "    models_test = []\n",
    "\n",
    "    for (w0,w1,w2) in model_weights:\n",
    "        # first make the model with empty weights\n",
    "        new_model = copy.deepcopy(hypotheses[0].model)\n",
    "        new_model.eval()\n",
    "        new_weight_dict = copy.deepcopy(weights_h[0])\n",
    "        for key in weights_h[0]:\n",
    "            new_weight_dict[key] = w0*weights_h[0][key] + w1*weights_h[1][key] + w2*weights_h[2][key]\n",
    "        new_model.load_state_dict(new_weight_dict)\n",
    "        models_test += [new_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for learner_id, learner in enumerate(aggregator.global_learners_ensemble):\n",
    "    print(learner_id)\n",
    "    print(learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learners = [client.learners_ensemble[learner_id] for client in aggregator.clients]\n",
    "len(learners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_learner = learner\n",
    "learners = learners\n",
    "\n",
    "target_state_dict = target_learner.model.state_dict(keep_vars=True)\n",
    "for key in target_state_dict:\n",
    "    print(key)\n",
    "    if target_state_dict[key].data.dtype == torch.float32:\n",
    "        \n",
    "        distance_matrix = np.zeros([len(learners),len(learners)])\n",
    "        state_dict_vals_log = []\n",
    "        \n",
    "        for learner_id, learner in enumerate(learners):\n",
    "            state_dict_vals_log += [learner.model.state_dict(keep_vars=True)[key].cpu().detach().numpy()]\n",
    "            \n",
    "        for i1,i2 in itertools.product(range(len(learners)),range(len(learners))):\n",
    "            if i1 != i2 and distance_matrix[i1, i2] == 0:\n",
    "                c = state_dict_vals_log[i1]- state_dict_vals_log[i2]\n",
    "                distance_matrix[i1, i2] = np.linalg.norm(c)\n",
    "                distance_matrix[i2, i1] = np.linalg.norm(c)\n",
    "    \n",
    "        # from distance matrix calculate mean for n-f-2, n=num learners, f = num_sybl, 2\n",
    "        krum_vector = np.zeros(len(learners))\n",
    "        # value of k\n",
    "        k = len(learners) - f - 2  \n",
    "\n",
    "        # using np.argpartition()\n",
    "        for i in range(len(learners)):\n",
    "            result = np.argpartition(distance_matrix[i], k)\n",
    "            krum_vector[i] = np.sum(distance_matrix[i][result[:k]])\n",
    "            \n",
    "        # Update weight using krum \n",
    "        min_idx = np.argmin(krum_vector)\n",
    "        target_state_dict[key].data = learners[min_idx].model.state_dict(keep_vars=True)[key].data.clone()\n",
    "        \n",
    "    else:\n",
    "        # tracked batches\n",
    "        target_state_dict[key].data.fill_(0)\n",
    "        for learner_id, learner in enumerate(learners):\n",
    "            state_dict = learner.model.state_dict()\n",
    "            target_state_dict[key].data += state_dict[key].data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr =np.random.uniform(0,1,[len(learners),len(learners)])\n",
    "\n",
    "krum_vector = np.zeros(len(learners))\n",
    "\n",
    "# value of k\n",
    "n = 40\n",
    "f = 10\n",
    "k = n-f-2  \n",
    "    \n",
    "# using np.argpartition()\n",
    "for i in range(len(learners)):\n",
    "    result = np.argpartition(arr[i], k)\n",
    "    krum_vector[i] = np.sum(arr[i][result[:k]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krum_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learners[0].model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learners[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedEM_env",
   "language": "python",
   "name": "fedem_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
