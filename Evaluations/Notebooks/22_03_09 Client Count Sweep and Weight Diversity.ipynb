{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/ubuntu/FedEM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import General Libraries\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import FedEM based Libraries\n",
    "from utils.utils import *\n",
    "from utils.constants import *\n",
    "from utils.args import *\n",
    "from run_experiment import *\n",
    "from models import *\n",
    "\n",
    "# Import Transfer Attack\n",
    "from transfer_attacks.Personalized_NN import *\n",
    "from transfer_attacks.Params import *\n",
    "from transfer_attacks.Transferer import *\n",
    "from transfer_attacks.Args import *\n",
    "from transfer_attacks.TA_utils import *\n",
    "from transfer_attacks.Boundary_Transferer import *\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Empty Aggregator to be loaded \n",
    "\n",
    "setting = 'FedEM'\n",
    "\n",
    "if setting == 'FedEM':\n",
    "    nL = 3\n",
    "else:\n",
    "    nL = 1\n",
    "    \n",
    "# Manually set argument parameters\n",
    "args_ = Args()\n",
    "args_.experiment = \"cifar10\"\n",
    "args_.method = setting\n",
    "args_.decentralized = False\n",
    "args_.sampling_rate = 1.0\n",
    "args_.input_dimension = None\n",
    "args_.output_dimension = None\n",
    "args_.n_learners= nL\n",
    "args_.n_rounds = 10\n",
    "args_.bz = 128\n",
    "args_.local_steps = 1\n",
    "args_.lr_lambda = 0\n",
    "args_.lr =0.03\n",
    "args_.lr_scheduler = 'multi_step'\n",
    "args_.log_freq = 10\n",
    "args_.device = 'cuda'\n",
    "args_.optimizer = 'sgd'\n",
    "args_.mu = 0\n",
    "args_.communication_probability = 0.1\n",
    "args_.q = 1\n",
    "args_.locally_tune_clients = False\n",
    "args_.seed = 1234\n",
    "args_.verbose = 1\n",
    "args_.save_path = 'weights/cifar/dummy/'\n",
    "args_.validation = False\n",
    "\n",
    "# Generate the dummy values here\n",
    "try:\n",
    "    aggregator\n",
    "except:\n",
    "    aggregator, clients = dummy_aggregator(args_, num_user=40)\n",
    "\n",
    "try:\n",
    "    dataloader\n",
    "except:\n",
    "    # Compiling Dataset from Clients\n",
    "    # Combine Validation Data across all clients as test\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "\n",
    "    for i in range(len(clients)):\n",
    "        daniloader = clients[i].test_iterator\n",
    "        for (x,y,idx) in daniloader.dataset:\n",
    "            data_x.append(x)\n",
    "            data_y.append(y)\n",
    "\n",
    "    data_x = torch.stack(data_x)\n",
    "    try:\n",
    "        data_y = torch.stack(data_y)        \n",
    "    except:\n",
    "        data_y = torch.FloatTensor(data_y) \n",
    "\n",
    "    dataloader = Custom_Dataloader(data_x, data_y)\n",
    "    global_dataloader = copy.deepcopy(dataloader)\n",
    "    \n",
    "# del aggregator, clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_test(directory_name, num_models):\n",
    "    # Generate the dummy values here\n",
    "    aggregator, clients = dummy_aggregator(args_, num_user=num_models)\n",
    "    \n",
    "    # Combined version\n",
    "    args_.save_path = directory_name\n",
    "    weights = np.load(directory_name+'train_client_weights.npy')\n",
    "    aggregator.load_state(args_.save_path)\n",
    "    \n",
    "    # This is where the models are stored -- one for each mixture --> learner.model for nn\n",
    "    hypotheses = aggregator.global_learners_ensemble.learners\n",
    "\n",
    "    # obtain the state dict for each of the weights \n",
    "    weights_h = []\n",
    "    \n",
    "    for h in hypotheses:\n",
    "        weights_h += [h.model.state_dict()]\n",
    "\n",
    "    # Set model weights\n",
    "    model_weights = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        model_weights += [weights[i]]\n",
    "\n",
    "    # Generate the weights to test on as linear combinations of the model_weights\n",
    "    models_test = []\n",
    "\n",
    "    for (w0,w1,w2) in model_weights:\n",
    "        # first make the model with empty weights\n",
    "        new_model = copy.deepcopy(hypotheses[0].model)\n",
    "        new_model.eval()\n",
    "        new_weight_dict = copy.deepcopy(weights_h[0])\n",
    "        for key in weights_h[0]:\n",
    "            new_weight_dict[key] = w0*weights_h[0][key] + w1*weights_h[1][key] + w2*weights_h[2][key]\n",
    "        new_model.load_state_dict(new_weight_dict)\n",
    "        models_test += [new_model]\n",
    "        \n",
    "    # For h_s\n",
    "    model_weights = [(1,0,0), (0,1,0), (0,0,1)]\n",
    "    h_s = []\n",
    "    for (w0,w1,w2) in model_weights:\n",
    "        # first make the model with empty weights\n",
    "        new_model = copy.deepcopy(hypotheses[0].model)\n",
    "        new_model.eval()\n",
    "        new_weight_dict = copy.deepcopy(weights_h[0])\n",
    "        for key in weights_h[0]:\n",
    "            new_weight_dict[key] = w0*weights_h[0][key] + w1*weights_h[1][key] + w2*weights_h[2][key]\n",
    "        new_model.load_state_dict(new_weight_dict)\n",
    "        h_s += [new_model]\n",
    "    \n",
    "    del aggregator\n",
    "    \n",
    "    return weights, models_test, h_s, clients\n",
    "\n",
    "def generate_empty_logs_adv(num_models):\n",
    "    \n",
    "    # Here we will make a dictionary that will hold results\n",
    "    logs_adv = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        adv_dict = {}\n",
    "        adv_dict['orig_acc_transfers'] = None\n",
    "        adv_dict['orig_similarities'] = None\n",
    "        adv_dict['adv_acc_transfers'] = None\n",
    "        adv_dict['adv_similarities_target'] = None\n",
    "        adv_dict['adv_similarities_untarget'] = None\n",
    "        adv_dict['adv_target'] = None\n",
    "        adv_dict['adv_miss'] = None\n",
    "        adv_dict['metric_alignment'] = None\n",
    "        adv_dict['ib_distance_legit'] = None\n",
    "        adv_dict['ib_distance_adv'] = None\n",
    "\n",
    "        logs_adv += [adv_dict]\n",
    "    \n",
    "    return logs_adv\n",
    "        \n",
    "# Perform transfer attack from one client to another and record stats\n",
    "\n",
    "def adv_atk(models_test, num_models, clients, custom_batch_size = 500):\n",
    "    \n",
    "    # Run Measurements for both targetted and untargeted analysis\n",
    "    new_num_models = len(models_test)\n",
    "    victim_idxs = range(new_num_models)\n",
    "    custom_batch_size = 500\n",
    "    eps = 4.5\n",
    "    \n",
    "    # Record number of batch sizes\n",
    "    batch_size_recs = np.ones([num_models,num_models])\n",
    "    victim_idxs = range(num_models)\n",
    "    logs_adv = generate_empty_logs_adv(num_models)\n",
    "    \n",
    "    print(\"Running Adv Attack\")\n",
    "    for adv_idx in victim_idxs:\n",
    "        print(\"\\t Adv idx:\", adv_idx)\n",
    "\n",
    "        dataloader = load_client_data(clients = clients, c_id = adv_idx, mode = 'test') # or test/train\n",
    "        batch_size = min(custom_batch_size, dataloader.y_data.shape[0])\n",
    "        batch_size_recs[adv_idx,:] *= batch_size\n",
    "\n",
    "        t1 = Transferer(models_list=models_test, dataloader=dataloader)\n",
    "        t1.generate_victims(victim_idxs)\n",
    "\n",
    "        # Perform Attacks Targeted\n",
    "        t1.atk_params = PGD_Params()\n",
    "        t1.atk_params.set_params(batch_size=batch_size, iteration = 10,\n",
    "                       target = 3, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x),\n",
    "                       step_size = 0.01, step_norm = \"inf\", eps = eps, eps_norm = 2)\n",
    "\n",
    "\n",
    "\n",
    "        t1.generate_advNN(adv_idx)\n",
    "        t1.generate_xadv(atk_type = \"pgd\")\n",
    "        t1.send_to_victims(victim_idxs)\n",
    "\n",
    "        # Log Performance\n",
    "        logs_adv[adv_idx]['orig_acc_transfers'] = copy.deepcopy(t1.orig_acc_transfers)\n",
    "        logs_adv[adv_idx]['orig_similarities'] = copy.deepcopy(t1.orig_similarities)\n",
    "        logs_adv[adv_idx]['adv_acc_transfers'] = copy.deepcopy(t1.adv_acc_transfers)\n",
    "        logs_adv[adv_idx]['adv_similarities_target'] = copy.deepcopy(t1.adv_similarities)        \n",
    "        logs_adv[adv_idx]['adv_target'] = copy.deepcopy(t1.adv_target_hit)\n",
    "\n",
    "        # Miss attack Untargeted\n",
    "        t1.atk_params.set_params(batch_size=batch_size, iteration = 10,\n",
    "                       target = -1, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x),\n",
    "                       step_size = 0.01, step_norm = \"inf\", eps = eps, eps_norm = 2)\n",
    "        t1.generate_xadv(atk_type = \"pgd\")\n",
    "        t1.send_to_victims(victim_idxs)\n",
    "        logs_adv[adv_idx]['adv_miss'] = copy.deepcopy(t1.adv_acc_transfers)\n",
    "        logs_adv[adv_idx]['adv_similarities_untarget'] = copy.deepcopy(t1.adv_similarities)\n",
    "        \n",
    "    return logs_adv\n",
    "\n",
    "def logs_adv_analysis(logs_adv):\n",
    "    \n",
    "    logs_analyzed_dict = {}\n",
    "    \n",
    "    new_num_models = len(logs_adv)\n",
    "    num_models = len(logs_adv)\n",
    "    \n",
    "    metrics = ['orig_acc_transfers','orig_similarities','adv_acc_transfers','adv_similarities_target',\n",
    "           'adv_similarities_untarget','adv_target','adv_miss'] #,'metric_alignment']\n",
    "\n",
    "    orig_acc = np.zeros([new_num_models, new_num_models]) \n",
    "    orig_sim = np.zeros([new_num_models, new_num_models]) \n",
    "    adv_acc = np.zeros([new_num_models, new_num_models]) \n",
    "    adv_sim_target = np.zeros([new_num_models, new_num_models]) \n",
    "    adv_sim_untarget = np.zeros([new_num_models, new_num_models]) \n",
    "    adv_target = np.zeros([new_num_models, new_num_models])\n",
    "    adv_miss = np.zeros([new_num_models, new_num_models]) \n",
    "\n",
    "    for adv_idx in range(num_models):\n",
    "        for victim in range(num_models):\n",
    "            orig_acc[adv_idx,victim] = logs_adv[adv_idx][metrics[0]][victim].data.tolist()\n",
    "            orig_sim[adv_idx,victim] = logs_adv[adv_idx][metrics[1]][victim].data.tolist()\n",
    "            adv_acc[adv_idx,victim] = logs_adv[adv_idx][metrics[2]][victim].data.tolist()\n",
    "            adv_sim_target[adv_idx,victim] = logs_adv[adv_idx][metrics[3]][victim].data.tolist()\n",
    "            adv_sim_untarget[adv_idx,victim] = logs_adv[adv_idx][metrics[4]][victim].data.tolist()\n",
    "            adv_target[adv_idx,victim] = logs_adv[adv_idx][metrics[5]][victim].data.tolist()\n",
    "            adv_miss[adv_idx,victim] = logs_adv[adv_idx][metrics[6]][victim].data.tolist()\n",
    "            \n",
    "        \n",
    "    logs_analyzed_dict['orig_acc'] = orig_acc\n",
    "    logs_analyzed_dict['orig_sim'] = orig_sim\n",
    "    logs_analyzed_dict['adv_acc'] = adv_acc\n",
    "    logs_analyzed_dict['adv_sim_target'] = adv_sim_target\n",
    "    logs_analyzed_dict['adv_sim_target'] = adv_sim_untarget\n",
    "    logs_analyzed_dict['adv_target'] = adv_target\n",
    "    logs_analyzed_dict['adv_miss'] = adv_miss\n",
    "    \n",
    "    return logs_analyzed_dict\n",
    "\n",
    "def measure_IB(models_test, dataloader, num_trials = 20, batch_size = 300):\n",
    "    \n",
    "    print(\"IB TEST\")\n",
    "    dists_measure_legit = np.zeros([len(models_test),num_trials, len(models_test)])\n",
    "    dists_measure_adv = np.zeros([len(models_test),num_trials, len(models_test)])\n",
    "\n",
    "    # attack parameters -- hard wired for now\n",
    "    eps = 4.5\n",
    "    iteration = 10\n",
    "    target = -1\n",
    "    eps_norm = 2\n",
    "    step_size = 0.01\n",
    "\n",
    "    t1 = Boundary_Transferer(models_list=models_test, dataloader=dataloader)\n",
    "\n",
    "    for j in range(len(models_test)):\n",
    "\n",
    "        for i in range(num_trials):\n",
    "            print(\"basemodel:\", j, \"IB num_trial:\", i)\n",
    "            t1.base_nn_idx = j\n",
    "            t1.victim_idx = range(len(models_test))\n",
    "\n",
    "            t1.atk_params = IFSGM_Params()\n",
    "            t1.atk_params.set_params(batch_size=1, eps=0.1, alpha=0.01, iteration = 30,\n",
    "                               target = -1, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x))\n",
    "\n",
    "            t1.set_adv_NN(t1.base_nn_idx)\n",
    "\n",
    "            base_ep_legit, victim_eps_legit = t1.legitimate_direction(batch_size=batch_size, ep_granularity = 0.1, \n",
    "                                                                      rep_padding = 1000, new_point = True,print_res = False)\n",
    "\n",
    "            base_ep_adv, victim_eps_adv = t1.adversarial_direction(ep_granularity = 0.1, \n",
    "                                                                      rep_padding = 1000, new_point = False,print_res = False)\n",
    "\n",
    "            idx = 0\n",
    "            for key, value in victim_eps_legit.items():\n",
    "                dists_measure_legit[j,i,idx] = np.abs(base_ep_legit-value)\n",
    "                idx+=1\n",
    "\n",
    "            idx = 0\n",
    "            for key, value in victim_eps_adv.items():\n",
    "                dists_measure_adv[j,i,idx] = np.abs(base_ep_adv - value)\n",
    "                idx+=1\n",
    "    \n",
    "    # Post filtering\n",
    "    # Filter data for adv and legit\n",
    "\n",
    "    for i in range(len(models_test)):\n",
    "        for j in range(dists_measure_legit[i].shape[0]):\n",
    "            if dists_measure_legit[i][j][i] > 0:\n",
    "                dists_measure_legit[i][j][i] = 0\n",
    "        for j in range(dists_measure_legit[i].shape[0]):\n",
    "            if dists_measure_adv[i][j][i] > 0:\n",
    "                dists_measure_adv[i][j][i] = 0\n",
    "\n",
    "#     ib_legit = np.mean(filtered_dists_measure_legit,axis=0)\n",
    "#     ib_adv = np.mean(filtered_dists_measure_adv,axis=0)\n",
    "    return dists_measure_legit, dists_measure_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = 'weights/cifar10/client_count/'\n",
    "directory_names = ['fedEM_c2/','fedEM_c4/','fedEM_c8/',\n",
    "                   #'fedEM_c10/', 'fedEM_c20/', 'fedEM_c40/',\n",
    "                   'fedEM_adv_c2/', 'fedEM_adv_c4/', 'fedEM_adv_c8/',\n",
    "                   #'fedEM_adv_c10/', 'fedEM_adv_c20/', 'fedEM_adv_c40/',\n",
    "                   'fedEM_dverge_c2/', 'fedEM_dverge_c4/', 'fedEM_dverge_c8/']\n",
    "                   #,'fedEM_dverge_c10/', 'fedEM_dverge_c20/', 'fedEM_dverge_c40/']\n",
    "\n",
    "num_models_list = [2,4,8,2,4,8,2,4,8]\n",
    "\n",
    "# IB test parameters\n",
    "ib_model_threshold = 10 # Number of models to use for IB analysis\n",
    "num_trials = 10\n",
    "batch_size = 200\n",
    "\n",
    "# Store values\n",
    "var_dict = {}\n",
    "orig_acc_dict = {}\n",
    "adv_miss_dict = {}\n",
    "ib_dist_dict = {}\n",
    "ib_dist_dict_compressed = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_dict = {}\n",
    "for i in range(len(directory_names)):\n",
    "     \n",
    "    print(\"EXP:\", directory_names[i][:-1])\n",
    "    weights, models_test, h_s, clients = get_models_test(base_name+directory_names[i], num_models_list[i])\n",
    "    logs_adv = adv_atk(models_test, num_models_list[i], clients, custom_batch_size = 500)\n",
    "    logs_dict[directory_names[i][:-1]] = logs_adv_analysis(logs_adv)\n",
    "    \n",
    "#     Interboundary dist\n",
    "    ib_leg, ib_adv = measure_IB(h_s, dataloader, num_trials=4)\n",
    "    \n",
    "    # Extract Values of Relevance\n",
    "    orig_acc = logs_dict[directory_names[i][:-1]]['orig_acc']\n",
    "    orig_acc_dict[directory_names[i][:-1]] = np.mean(np.diagonal(orig_acc))\n",
    "    \n",
    "    adv_miss = logs_dict[directory_names[i][:-1]]['adv_miss']\n",
    "    adv_miss_dict[directory_names[i][:-1]] = np.mean(np.diagonal(adv_miss))\n",
    "    \n",
    "    var = np.var(weights, axis=0)\n",
    "    var_dict[directory_names[i][:-1]] = var\n",
    "\n",
    "    ib_dist_dict[directory_names[i][:-1]] = (np.mean(ib_leg, axis=1), np.mean(ib_adv,axis=1))\n",
    "    ib_dist_dict_compressed[directory_names[i][:-1]] = (np.mean(ib_leg), np.mean(ib_adv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in var_dict:\n",
    "    var_mean = np.mean(var_dict[key])\n",
    "    print(key, var_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_miss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib_dist_dict_compressed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedEM_env",
   "language": "python",
   "name": "fedem_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
